{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIDzcATGVMZ3TeXmSXGTQZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anselmo-pitombeira/Notebooks/blob/master/Simple_NN_in_Numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of a simple neural network in Numpy"
      ],
      "metadata": {
        "id": "q_B87wzEDwC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import autograd.numpy as np\n",
        "import pandas as pd\n",
        "from autograd import grad\n"
      ],
      "metadata": {
        "id": "UK_VYNCrD1CV"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Read data\n",
        "dados = pd.read_csv(\"housing.csv\",  header=None, delim_whitespace=True)\n",
        "dados.columns = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B', 'LSTAT','MEDV']\n",
        "dados2 = dados[[\"CRIM\",\"INDUS\",\"NOX\",\"AGE\",\"TAX\"]]"
      ],
      "metadata": {
        "id": "YJKq8UvgEb0r"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBlmSWqHC9pb",
        "outputId": "fe8b68c0-9560-4978-c6ab-9be9b9cb9359"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1 =  [[ 0.310581   -0.68158347  0.41885685 -0.63987983 -0.92482791]\n",
            " [-0.34182128 -0.98773225 -0.04399259  0.84784888  0.40796912]\n",
            " [ 0.3849309   0.20957798 -0.5264861   0.83090452 -0.46488442]\n",
            " [-0.11758699 -0.70726251  0.84435375  0.55855202  0.08632298]\n",
            " [ 0.55987148 -0.94767378  0.76353469  0.53034073  0.53516492]\n",
            " [ 0.70549295 -0.81475155  0.70519321  0.56921146 -0.78906753]\n",
            " [ 0.12200465  0.8559842  -0.18857738  0.40782298  0.69776514]\n",
            " [ 0.24422107  0.21043034  0.34295125  0.56944705 -0.57867163]\n",
            " [ 0.77909649 -0.35047273 -0.48260386 -0.12865198  0.22870869]\n",
            " [-0.57775098 -0.68536643 -0.25320226  0.32158062 -0.5807368 ]]\n",
            "\n",
            "W2 =  [[-0.67065669  0.91475886 -0.93029911  0.54073808  0.48532503 -0.4218625\n",
            "  -0.54190818  0.78270808  0.80665493  0.82480373]]\n",
            "X shape =  (5, 506)\n",
            "Loss Grad shape =  (506, 1)\n"
          ]
        }
      ],
      "source": [
        "##Define activation function (RELu)\n",
        "def relu(x):\n",
        "\n",
        "  \"\"\"\n",
        "  A rectifier linear unit activation function.\n",
        "  \"\"\"\n",
        "\n",
        "  output = np.copy(x)\n",
        "  output[output<0] = 0\n",
        "\n",
        "  return output\n",
        "\n",
        "\n",
        "##Define forward computation\n",
        "def forward_pass(x,W1,W2,b1,b2):\n",
        "\n",
        "  \"\"\"\n",
        "  Forward computation of neural network.\n",
        "  \"\"\"\n",
        "  \n",
        "  ##Compute hidden layer\n",
        "  h = relu(W1.dot(x)+b1)\n",
        "\n",
        "  ##Compute output layer (just a linear function)\n",
        "  y = W2.dot(h)+b2\n",
        "  y = y.reshape(-1,1)\n",
        "\n",
        "  return y\n",
        "\n",
        "def loss_function(obs_y, pred_y):\n",
        "\n",
        "  \"\"\"\n",
        "  Average quadratic loss\n",
        "  \"\"\"\n",
        "\n",
        "  N = obs_y.size\n",
        "\n",
        "  loss = 1/N*np.sum((obs_y-pred_y)**2)\n",
        "\n",
        "  return loss\n",
        "\n",
        "##Size of input vector\n",
        "n = dados2.shape[1]\n",
        "\n",
        "##Desired number of neurons on hidden layer\n",
        "m = 10\n",
        "\n",
        "##Weight matrix of hidden layer (initialized as random)\n",
        "W1 = np.random.uniform(-1,1,(m,n))\n",
        "print(\"W1 = \", W1)\n",
        "\n",
        "##Bias vector of hidden layer\n",
        "b1 = np.random.uniform(-1,1,(m,1))\n",
        "\n",
        "##Weight matrix of output layer (output is univariate)\n",
        "W2 = np.random.uniform(-1,1,(1,m))\n",
        "print(\"\\nW2 = \", W2)\n",
        "\n",
        "##Bias scalar of output layer\n",
        "b2 = np.random.uniform(-1,1)\n",
        "\n",
        "\n",
        "X = dados2.values.transpose()\n",
        "print(\"X shape = \", X.shape)\n",
        "\n",
        "y_obs = dados[\"MEDV\"].values.reshape(-1,1)\n",
        "\n",
        "##Forward pass\n",
        "y_pred = forward_pass(X,W1,W2,b1,b2)\n",
        "\n",
        "##Compute loss\n",
        "\n",
        "loss = loss_function(y_pred,y_obs)\n",
        "\n",
        "loss_grad = grad(loss_function)\n",
        "\n",
        "print(\"Loss Grad shape = \", loss_grad(y_pred,y_obs).shape)\n"
      ]
    }
  ]
}